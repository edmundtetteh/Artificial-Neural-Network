{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c3b6a2-ba5d-43be-81c0-f7496f0f7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca60f861-c386-43a3-ac24-d33c62bad3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76fa1dbd-2a73-42de-af7d-db39cd4d14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    " \n",
    "# loss function\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3116c77-f263-4b8c-8951-5ff1ec33041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a parameters tensor\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd11418-4e29-4af3-b5cd-375930793795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(params.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b862789f-2a83-4265-af28-17bed1777976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing the argument for the lost function\n",
    "\n",
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward()\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eabf799c-1424-4646-a62e-5e205d20c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch would compute the derivatives of the loss throughout the chain of functions (the computation graph) and accumulate their values in the grad attribute of those tensors\n",
    "#  zero the gradient explicitly after using it for parameter updates.\n",
    "\n",
    "\n",
    "if params.grad is not None:\n",
    "    params.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b5e8fd8-ba36-4300-80b1-0e53084f5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autograd-enabled training\n",
    "\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:     \n",
    "            params.grad.zero_()\n",
    " \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    " \n",
    "        with torch.no_grad():     \n",
    "            params -= learning_rate * params.grad\n",
    " \n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    " \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca54b00-89e6-4a21-bef2-72f66612b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f5207f4-27ee-47b9-a380-b2ad0065c68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass argument in the autograd-enabled training\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0], requires_grad=True),    \n",
    "    t_u = t_un,    \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b6d3322-b782-47a3-8b83-227d214724a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    " \n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b8ad8c1-3213-4ff4-b9d6-4f191878682c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.7761, 0.1064], requires_grad=True),\n",
       " tensor(80.3643, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calling SGD(evaluating on subset of training sample (minibatch) instead of vanilla as use previously\n",
    "\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_un, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    " \n",
    "optimizer.step() # The value of params is updated upon calling step just like previous\n",
    "\n",
    "params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "750f586f-7740-46bd-af79-bedc7ab1d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  update our training loop accordingly: adding epoch\n",
    "\n",
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    " \n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d53c006d-b4a9-4bb7-b459-8e47123f7bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860120\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)    \n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,                           \n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "663124a5-47a2-4f59-aef2-be81fe034432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612900\n",
      "Epoch 1000, Loss 3.086698\n",
      "Epoch 1500, Loss 2.928578\n",
      "Epoch 2000, Loss 2.927646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([  0.5367, -17.3021], requires_grad=True),\n",
       " tensor(2.9276, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calling Adam ( very insensitive to scaling of parameter - â€‹so insensitive that we can go back to using the original (non-normalized) input t_u\n",
    "\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate)    \n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 2000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_u,    \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae610a20-6ea1-4f14-9787-7099caaf31c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'ANN/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls ANN/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
